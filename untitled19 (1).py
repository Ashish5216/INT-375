# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L-WIfh5Qf7MvF03PJaB87sx6ZIeKxPlK
"""

import pandas as pd
import numpy as np
df = pd.read_csv('/content/drive/MyDrive/adult_income_bias_dataset_300.csv')
df.head()

df.isnull().sum()
(df == ' ').sum()
df_cleaned = df.dropna()

from sklearn.impute import SimpleImputer

num_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']
imputer = SimpleImputer(strategy='mean')
df[num_cols] = imputer.fit_transform(df[num_cols])

cat_cols = ['workclass', 'education', 'marital-status', 'occupation',
            'relationship', 'race', 'sex', 'native-country', 'income']

cat_imputer = SimpleImputer(strategy='most_frequent')
df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])
df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)
df = df[df['hours-per-week'] <= 100]
df.head()

from sklearn.model_selection import train_test_split
X = df.drop(columns=['income'])
y = df['income']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('/content/drive/MyDrive/adult_income_bias_dataset_300.csv')

# Basic overview
print(df.shape)
print(df.info())
print(df.describe())

# Check income distribution
print(df['income'].value_counts())
sns.countplot(data=df, x='income')
plt.title("Income Class Distribution")
plt.show()

# Countplot for categorical variables
categorical_features = ['sex', 'race', 'education', 'marital-status', 'occupation']

for feature in categorical_features:
    plt.figure(figsize=(6, 3))
    sns.countplot(data=df, x=feature, hue='income')
    plt.title(f'{feature} vs Income')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# Histograms for numerical features
numerical_features = ['age', 'hours-per-week', 'capital-gain', 'capital-loss']

df[numerical_features].hist(bins=20, figsize=(10, 8))
plt.suptitle("Distributions of Numerical Features")
plt.tight_layout()
plt.show()

# Boxplots
for feature in numerical_features:
    plt.figure()
    sns.boxplot(x='income', y=feature, data=df)
    plt.title(f'{feature} vs Income')
    plt.show()

# Encode target for correlation analysis
df_corr = df.copy()
df_corr['income'] = df_corr['income'].map({'<=50K': 0, '>50K': 1})

# Select only numerical features for correlation analysis
numerical_features = ['age', 'hours-per-week', 'capital-gain', 'capital-loss']
df_corr_num = df_corr[numerical_features + ['income']]

# Heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df_corr_num.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# Compare mean values grouped by income
print(df.groupby('income')[numerical_features].mean())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('/content/drive/MyDrive/adult_income_bias_dataset_300.csv')

# Encode categorical columns
label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

# Features and target
X = df.drop('income', axis=1)
y = df['income']

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoders['income'].classes_, yticklabels=label_encoders['income'].classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Optional: Print classification report
print(classification_report(y_test, y_pred))

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load dataset
df = pd.read_csv('/content/drive/MyDrive/adult_income_bias_dataset_300.csv')

# Step 2: Encode categorical columns
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Step 3: Separate features and target
X = df.drop('income', axis=1)  # change if target column is different
y = df['income']

# Step 4: Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 5: Apply PCA
pca = PCA(n_components=0.95)  # keep 95% variance
X_pca = pca.fit_transform(X_scaled)

# Step 6: Output results
print(f'Original number of features: {X.shape[1]}')
print(f'Reduced number of features with PCA: {X_pca.shape[1]}')

# Optional: Plot explained variance
plt.figure(figsize=(10,6))
sns.lineplot(x=range(1, len(pca.explained_variance_ratio_)+1), y=pca.explained_variance_ratio_.cumsum(), marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by PCA Components')
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('/content/drive/MyDrive/adult_income_bias_dataset_300.csv')

# Encode categorical columns
label_encoders = {}
for col in df.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Split features and target
X = df.drop('income', axis=1)
y = df['income']

# Standardize features (important for regularization)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Apply Logistic Regression with L2 regularization (default)
model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')  # Lower C = stronger regularization
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix with L2 Regularization")
plt.show()

# Classification report
print(classification_report(y_test, y_pred))

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Example data
x = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([2, 4, 5, 4, 5])

# Fit linear regression model
model = LinearRegression()
model.fit(x, y)

# Get slope (m) and intercept (b)
m = model.coef_[0]
b = model.intercept_

print(f"Slope (m): {m}")
print(f"Intercept (b): {b}")

# Predict y using the model
y_pred = model.predict(x)

# Plot the data and the line
plt.scatter(x, y, color='blue', label='Data points')
plt.plot(x, y_pred, color='red', label=f'y = {m:.2f}x + {b:.2f}')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Linear Regression: y = mx + b')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier

# Step 1: Load dataset
df = pd.read_csv('/content/drive/MyDrive/adult_income_bias_dataset_300.csv')

# Step 2: Encode categorical features
label_encoders = {}
for col in df.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Step 3: Separate features and target
X = df.drop('income', axis=1)  # Update if your target column has a different name
y = df['income']

# Step 4: Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 5: Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Step 6: Train XGBoost model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)

# Step 7: Predictions
y_pred = xgb_model.predict(X_test)

# Step 8: Evaluation
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Evaluation metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (RÂ²): {r2:.4f}")

# Step 2: Choose numerical columns (replace with appropriate ones from your data)
# We'll use 'hours-per-week' to predict 'age' just as an example
X = df['hours-per-week'].values
y = df['age'].values

# Step 3: Normalize X for better convergence
X = (X - np.mean(X)) / np.std(X)

# Step 4: Initialize parameters
m = 0
b = 0
learning_rate = 0.01
epochs = 1000
n = len(X)

# Step 5: Gradient Descent loop
loss_history = []

for i in range(epochs):
    y_pred = m * X + b
    error = y_pred - y

    # Gradients
    dm = (2/n) * np.dot(error, X)
    db = (2/n) * np.sum(error)

    # Update parameters
    m -= learning_rate * dm
    b -= learning_rate * db

    # Calculate and store loss (MSE)
    mse = np.mean(error ** 2)
    loss_history.append(mse)

    if i % 100 == 0:
        print(f"Epoch {i}: MSE = {mse:.4f}, m = {m:.4f}, b = {b:.4f}")

# Final results
print("\nFinal Model: y = {:.2f}x + {:.2f}".format(m, b))

# Plot: Regression Line
plt.figure(figsize=(10,5))
plt.scatter(X, y, color='blue', alpha=0.5, label='Data points')
plt.plot(X, m*X + b, color='red', label='Fitted line')
plt.title('Linear Regression using Gradient Descent')
plt.xlabel('Normalized hours-per-week')
plt.ylabel('Age')
plt.legend()
plt.grid(True)
plt.show()

# Plot: Loss curve
plt.figure(figsize=(8,4))
plt.plot(loss_history, color='green')
plt.title("Loss over Epochs")
plt.xlabel("Epochs")
plt.ylabel("MSE Loss")
plt.grid(True)
plt.show()

# Choose numerical columns (replace with appropriate ones from your data)
X = df['hours-per-week'].values
y = df['age'].values

# Normalize the feature
X = (X - np.mean(X)) / np.std(X)

# Cross-validation parameters
k_folds = 5
learning_rate = 0.01
epochs = 1000
n = len(X)

# K-Fold Cross Validation
kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)

# Store the MSE for each fold
mse_folds = []

# Gradient Descent for each fold
for fold, (train_index, val_index) in enumerate(kf.split(X), 1):
    # Split the data into train and validation sets
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # Initialize parameters for gradient descent
    m, b = 0, 0

    # Gradient Descent Loop
    for epoch in range(epochs):
        # Predictions
        y_pred = m * X_train + b
        error = y_pred - y_train

        # Gradients
        dm = (2 / len(X_train)) * np.dot(error, X_train)
        db = (2 / len(X_train)) * np.sum(error)

        # Update parameters
        m -= learning_rate * dm
        b -= learning_rate * db

    # Validate the model
    y_val_pred = m * X_val + b
    mse = mean_squared_error(y_val, y_val_pred)
    mse_folds.append(mse)

    print(f"Fold {fold}: MSE = {mse:.4f}")

# Final Cross-Validation Results
avg_mse = np.mean(mse_folds)
print(f"\nAverage MSE across {k_folds} folds: {avg_mse:.4f}")

# Optionally: Plot MSE across folds
plt.figure(figsize=(8, 5))
plt.plot(range(1, k_folds+1), mse_folds, marker='o', color='blue')
plt.title('MSE for each Fold in Cross-Validation')
plt.xlabel('Fold')
plt.ylabel('MSE')
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np

# Step 1: Load the dataset
df = pd.read_csv('/content/drive/MyDrive/adult_income_bias_dataset_300.csv')

# Step 2: Choose two numerical columns for analysis (replace with actual columns)
# Example: let's use 'hours-per-week' and 'age'
X = df['hours-per-week'].values
Y = df['age'].values

# Step 3: Calculate Covariance
# Covariance of X and Y
cov_XY = np.cov(X, Y)[0, 1]
print(f"Covariance between 'hours-per-week' and 'age': {cov_XY:.4f}")

# Step 4: Calculate Correlation
# Pearson correlation coefficient
correlation_XY = np.corrcoef(X, Y)[0, 1]
print(f"Correlation between 'hours-per-week' and 'age': {correlation_XY:.4f}")

# Optionally: You can also use pandas to calculate these easily

# Covariance using pandas
cov_pandas = df[['hours-per-week', 'age']].cov().iloc[0, 1]
print(f"\nCovariance (using pandas): {cov_pandas:.4f}")

# Correlation using pandas
correlation_pandas = df[['hours-per-week', 'age']].corr().iloc[0, 1]
print(f"Correlation (using pandas): {correlation_pandas:.4f}")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from plotly.express import scatter, box

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/adult_income_bias_dataset_300.csv')

# Step 1: Display Basic Information about the dataset
print("Dataset Info:")
df.info()

print("\nSummary Statistics:")
print(df.describe())

# Step 2: Univariate Analysis (Distribution of Features)
plt.figure(figsize=(12, 6))
sns.histplot(df['hours-per-week'], kde=True, color='blue', bins=20)
plt.title('Distribution of Hours per Week')
plt.xlabel('Hours per Week')
plt.ylabel('Frequency')
plt.show()

# Step 3: Visualize categorical variables
# Let's assume 'income' is a categorical feature (income >50K or <=50K)
plt.figure(figsize=(6, 4))
sns.countplot(x='income', data=df, palette='Set2')
plt.title('Income Distribution (<=50K vs >50K)')
plt.xlabel('Income')
plt.ylabel('Count')
plt.show()

# Step 4: Bivariate Analysis (Scatter plot for relationships between numerical features)
plt.figure(figsize=(12, 6))
sns.scatterplot(x='hours-per-week', y='age', data=df, color='red')
plt.title('Scatter Plot: Hours per Week vs Age')
plt.xlabel('Hours per Week')
plt.ylabel('Age')
plt.show()

# Step 6: Box Plot (for detecting outliers)
plt.figure(figsize=(12, 6))
sns.boxplot(x='income', y='hours-per-week', data=df, palette='Set1')
plt.title('Box Plot: Income vs Hours per Week')
plt.xlabel('Income')
plt.ylabel('Hours per Week')
plt.show()

# Step 7: Pairplot (visualizing pairwise relationships between features)
sns.pairplot(df[['hours-per-week', 'age', 'education-num', 'capital-gain']], hue='income')
plt.suptitle("Pairplot of Features")
plt.show()

# Step 8: Missing Data Visualization
plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Data Heatmap')
plt.show()

# Step 9: Visualizing distributions using Plotly (Interactive)
scatter(df, x='hours-per-week', y='age', color='income', title="Interactive Scatter Plot", labels={'hours-per-week': 'Hours per Week', 'age': 'Age'})

# Show interactive plot

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from plotly.express import scatter, box

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/adult_income_bias_dataset_300.csv')

# ... (Your existing code for steps 1-4) ...

# Step 5: Correlation Heatmap
# Select only numerical features for correlation analysis
numerical_features = df.select_dtypes(include=np.number).columns  # Select numerical columns

# Calculate the correlation matrix for numerical features only
corr = df[numerical_features].corr()

plt.figure(figsize=(12, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

# ... (Rest of your code) ...